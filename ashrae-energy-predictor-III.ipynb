{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASHRAE - Great Energy Predictor III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feather-format in c:\\users\\hanti\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: pyarrow>=0.4.0 in c:\\users\\hanti\\anaconda3\\lib\\site-packages (from feather-format) (0.15.1)\n",
      "Requirement already satisfied: six>=1.0.0 in c:\\users\\hanti\\anaconda3\\lib\\site-packages (from pyarrow>=0.4.0->feather-format) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\hanti\\anaconda3\\lib\\site-packages (from pyarrow>=0.4.0->feather-format) (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install feather-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict, cross_val_score\n",
    "import math\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Convert Pandas Dataframe to Feather\n",
    "\n",
    "We use a framework called feather for more efficient read and write operations in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data...\n",
    "#root = '/kaggle/input/ashrae-energy-prediction'\n",
    "root = './ashrae-energy-prediction'\n",
    "\n",
    "# Read from csv files\n",
    "train_df = pd.read_csv(os.path.join(root, 'train.csv'))\n",
    "weather_train_df = pd.read_csv(os.path.join(root, 'weather_train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(root, 'test.csv'))\n",
    "weather_test_df = pd.read_csv(os.path.join(root, 'weather_test.csv'))\n",
    "building_meta_df = pd.read_csv(os.path.join(root, 'building_metadata.csv'))\n",
    "sample_submission = pd.read_csv(os.path.join(root, 'sample_submission.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save data in feather format\n",
    "train_df.to_feather('train.feather')\n",
    "test_df.to_feather('test.feather')\n",
    "weather_train_df.to_feather('weather_train.feather')\n",
    "weather_test_df.to_feather('weather_test.feather')\n",
    "building_meta_df.to_feather('building_metadata.feather')\n",
    "sample_submission.to_feather('sample_submission.feather')\n",
    "\n",
    "# Delete raw data from memory for better performance\n",
    "del train_df\n",
    "del test_df\n",
    "del weather_train_df\n",
    "del weather_test_df\n",
    "del building_meta_df\n",
    "del sample_submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_feather('train.feather')\n",
    "weather_train = pd.read_feather('weather_train.feather')\n",
    "building_meta = pd.read_feather('building_metadata.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Remove Outliers\n",
    "\n",
    "Building 1099 has abnormally higher metering readings than the other buildings.\n",
    "\n",
    "Buildings with id <= 104 has meter readings 0 before 05/20/2016, which is anomaly low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "train = train[train['building_id'] != 1099]\n",
    "train = train.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Scale Square Feet\n",
    "\n",
    "Since square_feet spans several order of magnitude(some are very big and some are very small, we take the log for better comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta['square_feet'] =  np.log1p(building_meta['square_feet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Handle Missing Values \n",
    "\n",
    "For accuracy and least interference with model fitting, we first group the data base on site_id, month, and day. And fill missing with the daily average in a site.\n",
    "\n",
    "For features such as 'air_temperature' and 'dew_temperature', we can fill both directions because they follow a normal distribution. (We can fill backwards)\n",
    "\n",
    "For 'cloud_coverage', 'sea_level_pressure', 'precip_depth_1_hr', however, we only fill forward because next observation is not indicative of prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def weather_pipeline(weather):\n",
    "    \n",
    "    # Count total number of hours in dataset\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start = datetime.datetime.strptime(weather['timestamp'].min(),time_format)\n",
    "    end = datetime.datetime.strptime(weather['timestamp'].max(),time_format)\n",
    "    all_hours = int(((end - start).total_seconds()) / 3600 + 1)\n",
    "    \n",
    "    \n",
    "    timestamps = [(end - datetime.timedelta(hours=x)).strftime(time_format) for x in range(all_hours)]\n",
    "    \n",
    "    \n",
    "    # Add missing timestamps for each site\n",
    "    for site_id in range(16):\n",
    "        split_site_hours = np.array(weather[weather['site_id'] == site_id]['timestamp'])\n",
    "        # Get missing hours\n",
    "        missing_hours = pd.DataFrame(np.setdiff1d(timestamps, split_site_hours), columns=['timestamp'])\n",
    "        missing_hours['site_id'] = site_id\n",
    "        weather = pd.concat([weather, missing_hours])\n",
    "        weather = weather.reset_index(drop=True) \n",
    "    \n",
    "\n",
    "        \n",
    "    weather[\"datetime\"] = pd.to_datetime(weather[\"timestamp\"])\n",
    "    weather[\"day\"] = weather[\"datetime\"].dt.day\n",
    "    weather[\"week\"] = weather[\"datetime\"].dt.week\n",
    "    weather[\"month\"] = weather[\"datetime\"].dt.month\n",
    "    \n",
    "    weather = weather.set_index(['site_id','day','month'])\n",
    "    \n",
    "    \n",
    "    normal = ['air_temperature', 'dew_temperature', 'wind_direction', 'wind_speed']\n",
    "    forward = ['cloud_coverage', 'sea_level_pressure', 'precip_depth_1_hr']\n",
    "    \n",
    "    # Fill both directions with mean\n",
    "    for item in normal:\n",
    "        feature_filler = pd.DataFrame(weather.groupby(['site_id','day','month'])[item].mean(), columns=[item])\n",
    "        weather.update(feature_filler, overwrite=False)\n",
    "    \n",
    "    \n",
    "    # Fill forward \n",
    "    for item in forward:\n",
    "        feature_filler = weather.groupby(['site_id','day','month'])[item].mean()\n",
    "        feature_filler = pd.DataFrame(feature_filler.fillna(method='ffill'),columns=[item])\n",
    "        weather.update(feature_filler,overwrite=False)\n",
    "\n",
    "    weather = weather.reset_index()\n",
    "    weather = weather.drop(['datetime','day','week','month'],axis=1)\n",
    "        \n",
    "    return weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_train = weather_pipeline(weather_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4 Memory Reduction and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(train,use_float16=True)\n",
    "building_meta = reduce_mem_usage(building_meta,use_float16=True)\n",
    "weather_train = reduce_mem_usage(weather_train,use_float16=True)\n",
    "\n",
    "train = train.merge(building_meta, left_on='building_id',right_on='building_id',how='left')\n",
    "train = train.merge(weather_train, how='left', left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])\n",
    "del weather_train\n",
    "del building_meta\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.5 Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def FE_pipeline(data):\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    data.sort_values(\"timestamp\")\n",
    "    data.reset_index(drop=True)\n",
    "    \n",
    "    # Add more features\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    data[\"hour\"] = data[\"timestamp\"].dt.hour\n",
    "    data[\"day_of_week\"] = data[\"timestamp\"].dt.weekday\n",
    "    \n",
    "    \n",
    "    # Remove Unused Columns\n",
    "    feature_to_drop = [\"timestamp\",\n",
    "                       \"sea_level_pressure\", \n",
    "                       \"wind_direction\", \n",
    "                       \"wind_speed\",\n",
    "                       \"year_built\",\n",
    "                       \"floor_count\"]\n",
    "    data = data.drop(feature_to_drop, axis=1)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Encode Categorical Data\n",
    "    labelencoder = LabelEncoder()\n",
    "    data[\"primary_use\"] = labelencoder.fit_transform(data[\"primary_use\"])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = FE_pipeline(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.6 Fix Unit Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix unit error \n",
    "train.loc[(train['site_id']==0) & (train['meter']==0), 'meter_reading'] = train.loc[(train['site_id']==0)&(train['meter']==0), 'meter_reading'] * 0.2931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[(train.site_id==0)&(train.meter==0)].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Scale Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(train[\"meter_reading\"])\n",
    "X = train.drop('meter_reading', axis = 1)\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Stratified Train-Test Split\n",
    "Since we are training by site, we need to split evenly by site_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test with equal percentage of site id\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle = True, stratify=X['site_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gradient Boosting, Divide & Conquer, and RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I.\tModel Training\n",
    "    - a.\tAlgorithm: Gradient Boosting\n",
    "        - i.\tGradient Boosting is an ensemble machine learning technique that produces a prediction model by building an ensemble/sequence of weak prediction models (decision trees).\n",
    "        - ii.\tUnlike AdaBoost, which uses stumps, Gradient Boosting builds trees in a stage-wise manner. The general idea of the algorithm is as follow\n",
    "            - 1.\tUse the average of target variables as prediction for every instance;\n",
    "            - 2.\tCompute the Pseudo Residual = Observed – Prediction for each instance;\n",
    "            - 3.\tBuild a decision tree using input variables with residuals as leaves at the lowest level; (If there are multiple residuals in one leaf, take the average of them)\n",
    "            - 4.\tMake a new prediction = Average Observed + Learning Rate x Residual for each instance; We use a Learning Rate to scale in order to prevent high variance(overfitting); Learning Rate results in a small step in the right direction. The model will perform better on testing dataset.\n",
    "            - 5.\tCompute the new Pseudo Residual = (Observed – (Average Observed + Sum of (Learning Rate x Previous Residuals))\n",
    "            - 6.\tBuild a new tree based on new residuals.\n",
    "            - 7.\tRepeat until the maximum number of trees have been reached or additional trees fail to improve the fit.\n",
    "            \n",
    "        - iii. Pseudo Code\n",
    "            - Input: Data {(xi, yi)} for i from 1 to n, and a differentiable loss function(in our case, the RMSLE)\n",
    "            - Step 1: Initialize model with a constant value\n",
    "                      \n",
    "                $$F_0(x) = \\operatorname*{argmin}_\\gamma \\sum_{i=1}^{n} L(y_i, \\gamma)$$\n",
    "            <br><br>\n",
    "            - Step 2: For m = 1 to M where m is the index of tree and M is the maximum number of trees\n",
    "                - (A) Compute:\n",
    "$$r_{i,m} = -[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x) = F_{m-1}(x)}$$\n",
    "                    \n",
    "                     for i = 1, ... , n\n",
    "                <br><br>\n",
    "                - (B) Fit regression tree to the $r_{i,m}$ values and create terminal regions $R_{j,m}$, for j = 1...$J_m$ where j is the leaf index and $J_m$ is the number of leaves in tree m\n",
    "                <br> <br>     \n",
    "                - (C) For j = 1...$J_m$, compute: \n",
    "                    $$\\gamma_{j, m} = \\operatorname*{argmin}_\\gamma \\sum_{x_i\\in{R_{i,j}}} L(y_i, F_{m-1}(x_i)+\\gamma)$$\n",
    "                <br> <br> \n",
    "                - (D) Update $$F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_{j,m} I(x\\in{R_{j,m}})$$ where $\\nu$ is the learning rate\n",
    "                <br><br>\n",
    "            \n",
    "    - b. Framework: LightGBM\n",
    "        - i. LightGBM was developed by Microsoft based on the idea of XGBoost. The difference between XGBoost and Naive Gradient Boosting which we described earlier is that XGBoost uses an objective Function\n",
    "$$Obj(\\theta) = L(\\theta) + \\Omega(\\theta)$$\n",
    "            where $L(\\theta)$ is the loss function we previously described and $\\Omega(\\theta)$ is the regularization.\n",
    "            <br>\n",
    "            <br>\n",
    "            Let's say we have $f_t(x) = w_q(x), w\\in{R^T}$, q: ${\\mathbb{R}}^d$ -> {1, 2, ... , Q} where w is the leaf weight of the tree and q is the structure of the tree, then\n",
    "            $$\\Omega(f_t) = \\frac{1}{2} \\times \\lambda(\\sum_{j=1}^{Q} {w_j}^{2}) + \\alpha(\\sum_{i=1}^{Q} |w_i|) + \\gamma Q$$\n",
    "            \n",
    "            where $\\lambda$ is XGBoost's lambda, $\\sum_{j=1}^{Q} {w_j}^{2}$ is L1 penalty, $\\alpha$ is XGboost's alpha, $\\sum_{i=1}^{Q} |w_i|$ is L2 penalty, $\\gamma$ is XGBoost's gamma, and Q is the number of leaves.\n",
    "            \n",
    "            <br>\n",
    "            So after combining the loss and regularization, the formula becomes\n",
    "$$ Obj(t) = \\sum_{i=1}^{n} (g_i\\times w_{q(x_i)} + \\frac{h_i}{2} \\times w_{q(x_i)}) + \\frac{1}{2} \\times \\lambda(\\sum_{j=1}^{Q} {w_j}^{2}) + \\alpha(\\sum_{i=1}^{Q} |w_i|) + \\gamma Q$$\n",
    "\n",
    "          $$= \\sum_{j=1}^{Q} [(\\sum_{i\\in{I_j}} g_i \\times w_{q(x_i)} + \\frac{(\\sum_{i\\in{I_j}} h_i) + \\lambda}{2} \\times w_{q_(x_i)} ^ 2] + \\gamma Q$$ \n",
    "          \n",
    "          where $I_j = \\{i|q(x_i) = k\\}$ is a set of indices of points assigned to j-th leaf, $g_i$ and $h_i$ are the gradient(first-degree) and hessian(second-degree) of the loss function.\n",
    "          <br>\n",
    "          For simplification, let\n",
    "          $$ G_j = \\sum_{i\\in{I_j}} g_i$$\n",
    "          $$ H_i = \\sum_{i\\in_{I_j}} h_i$$\n",
    "          <br>\n",
    "          Then, the optimal weight for a fixed tree is \n",
    "          $$w_j^* = - \\frac{G_j}{H_i+\\lambda}$$\n",
    "          <br>\n",
    "          LightGBM grows the tree greedily by computing the gain\n",
    "          $$ Gain = \\frac{1}{2} [\\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{(G_L+G_R)}{H_L+H_R+\\lambda}] - \\lambda$$\n",
    "          <br>\n",
    "          where $\\frac{G_L^2}{H_L+\\lambda}$ is the left child score, $\\frac{G_R^2}{H_R+\\lambda}$ is the right child score,  $\\frac{(G_L+G_R)}{H_L+H_R+\\lambda}$ is the score if we do not split, and $\\lambda$ is the complexity cost if we add a new split.\n",
    "          <br>\n",
    "          <br>\n",
    "          LightGBM finds the best split point by iterating over sorted attributes and compute the gain.\n",
    "          <br>\n",
    "          <br>\n",
    "      - ii. Some characteristics of LightGBM\n",
    "      <br>\n",
    "          - Tree is grown in Breadth-First fashion instead of Depth-First so that we can sort and traverse the data only once on each level\n",
    "          <br>\n",
    "          - Sorted features can be cached, so that we don't have to sort many times\n",
    "          <br>\n",
    "          - Each continuous feature is bucketed into discrete bins, and we iterate over number of bins instead of number of points\n",
    "    \n",
    "          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c. Training Methodology: Divide and Conquer\n",
    "    - We will split the data by site and train a separate model on each site. \n",
    "    - The reasons:\n",
    "        - 1. There are time zone differences of different sites.\n",
    "        - 2. The size of the data is very large and it is difficult to fit a single model that can generalize on multiple locations despite the advantages of gradient boosting.\n",
    "        - 3. Decrease the training time \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- d. Evaluation Metric: Since we have already logged the output variable, we can just use RMSE as the evaluation metric which will produce RMSLE.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict, cross_val_score\n",
    "import math\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns the RMSE of prediction values\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    y_true = y_true.values\n",
    "    terms_to_sum = 0\n",
    "    for i, pred in enumerate(y_pred):\n",
    "        prediction = y_pred[i] if y_pred[i] > 0 else 0 # Replace negative values with 0\n",
    "        terms_to_sum += (prediction - y_true[i]) ** 2.0\n",
    "    return ((terms_to_sum * (1.0/len(y_true))) ** 0.5)\n",
    "\n",
    "scorer = make_scorer(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 First Attempt\n",
    "We will just use default parameters for now and plot the distribution of our prediction to see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"day_of_week\"]\n",
    "\n",
    "for site_id in range(16):\n",
    "    print('Site:', site_id)\n",
    "    reg = lgb.LGBMRegressor(n_threads=16, verbose=0)\n",
    "    reg.fit(X_train[X_train.site_id==site_id], \n",
    "            y_train[X_train.site_id==site_id], \n",
    "            categorical_feature=categorical_features, \n",
    "            eval_metric='rmse', \n",
    "            verbose=False)\n",
    "    \n",
    "    y_pred = reg.predict(X_test[X_test.site_id==site_id])\n",
    "    \n",
    "    # Plot Distribution\n",
    "    sns.distplot(y_pred, label='Prediction')\n",
    "    sns.distplot(y_test[X_test.site_id==site_id], label='Ground Truth')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- Site 1, 2, 6, 7, 9, 10, 11, 13, 14, 15 all seem to have an abnormal amount of 0 meter_reading in ground truth which we fail to predict. \n",
    "- The model seems a little bit overfit. We are worried that it will not generalize well on the testing dataset which is twice as big as the training dataset.\n",
    "- Therefore, We will tune the parameters next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LightGBM parameters:\n",
    "    - 1. learning_rate: Multiplication factor on each boosting iteration; (0, 1]\n",
    "    - 2. n_estimators: Number of boosted tree to fit; Large value can decrease overfitting\n",
    "    - 3. num_leaves: Maximum Number of Leaves; Large value can improve accuracy but expose to overfitting. Typical value is 2 ^ max_depth\n",
    "    - 4. max_depth: Maximum depth of boosted tree; Too deep can cause overfitting\n",
    "    - 5. min_data_in_leaf: Very important parameter to prevent overfitting\n",
    "    - 6. bagging_fraction: Randomly select part of rows(instances); Can be used to prevent overfitting\n",
    "    - 7. feature_fraction: Selecting part of the columns(features); Can be used to deal with the curse of dimensionality \n",
    "    - 8. lambda_l1: l1 regularization\n",
    "    - 9. lambda_l2: l2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hyperopt Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe, fmin\n",
    "lgb_space = {'boosting': hp.choice('booster', ['gbdt', 'dart']),\n",
    "             'num_leaves': hp.choice('num_leaves', [32, 64, 128, 256, 512, 1024, 2048]),  # The number of leavers per tree\n",
    "             'learning_rate': hp.quniform('learning_rate', 0.001, 0.05, 0.001), # The learning rate of boosted tree\n",
    "             'max_depth': hp.quniform('max_depth', 5, 20, 1), # The maximum depths per tree(prevent overfitting)\n",
    "             'feature_fraction': hp.quniform('feature_fraction', 0.5, 1.0, 0.1), # Percentage of features to use for tree\n",
    "             'bagging_fraction': hp.quniform('bagging_fraction', 0.7, 1.0, 0.1), # Percentage of rows for tree\n",
    "             'min_data_in_leaf': hp.quniform('min_data_in_leaf', 20, 100, 10), # Minimum data in leaf(prevent overfitting) \n",
    "             'lambda_l1' : hp.loguniform('lambda_l1', np.log(0.1), np.log(10)), # l1 regularization(prevent overfitting)\n",
    "             'lambda_l2' : hp.loguniform('lambda_l2', np.log(0.1), np.log(10)), # l2 regularization(prevent overfitting)\n",
    "             'min_gain_to_split':hp.quniform('min_gain_to_split'0.0, 1.0, 0.1) # gamma\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()\n",
    "del train\n",
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning parameter for Site 0\n",
      " 10%|█████                                              | 1/10 [01:42<15:23, 102.62s/it, best loss: 1.1384693094481317]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-2b013c48b224>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                     algo=tpe.suggest)\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[0;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    225\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                         \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 844\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-116-2b013c48b224>\u001b[0m in \u001b[0;36mlgb_objective\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     30\u001b[0m                                                 'categorical_feature' : categorical_features}, \n\u001b[0;32m     31\u001b[0m                                      \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                                      \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;31m# 3 fold cross validation(67% training, 33% evaluation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                                      ).mean()\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    387\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 231\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[1;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[0;32m    595\u001b[0m     \"\"\"\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[1;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[0;32m    625\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \"\"\"\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m                              % (self._n_features, n_features))\n\u001b[0;32m    606\u001b[0m         return self.booster_.predict(X, raw_score=raw_score, num_iteration=num_iteration,\n\u001b[1;32m--> 607\u001b[1;33m                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[0;32m   2201\u001b[0m         return predictor.predict(data, num_iteration,\n\u001b[0;32m   2202\u001b[0m                                  \u001b[0mraw_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2203\u001b[1;33m                                  data_has_header, is_reshape)\n\u001b[0m\u001b[0;32m   2204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__pred_for_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m             \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__pred_for_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__pred_for_np2d\u001b[1;34m(self, mat, num_iteration, predict_type)\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0minner_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__pred_for_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36minner_predict\u001b[1;34m(mat, num_iteration, predict_type, preds)\u001b[0m\n\u001b[0;32m    536\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_parameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_num_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m                 preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n\u001b[0m\u001b[0;32m    539\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn_preds\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mout_num_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Wrong length for predict results\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "best_params = {} # Best parameter for each site\n",
    "\n",
    "for i in range(16):\n",
    "    def lgb_objective(params):\n",
    "        params = {'boosting': params['boosting'],\n",
    "                    'num_leaves': int(params['num_leaves']),\n",
    "                  'learning_rate': params['learning_rate'],\n",
    "                  'max_depth': int(params['max_depth']),\n",
    "                  'feature_fraction': params['feature_fraction'],\n",
    "                  'bagging_fraction': params['bagging_fraction'],\n",
    "                  'min_data_in_leaf': int(params['min_data_in_leaf']),\n",
    "                  'lambda_l1' : params['lambda_l1'],\n",
    "                  'lambda_l2' : params['lambda_l2']}\n",
    "\n",
    "        gbm_reg = lgb.LGBMRegressor(n_estimators=1000, \n",
    "                                    #early_stopping_round=50,\n",
    "                                    n_threads=16, # Multithreading to speed up training\n",
    "                                    #verbose=-1,\n",
    "                                    **params) # Pass in the parameter space\n",
    "\n",
    "        best_score = cross_val_score(gbm_reg, \n",
    "                                     X[X.site_id==i], \n",
    "                                     y[X.site_id==i], \n",
    "                                     fit_params={'verbose' : False,\n",
    "                                                'categorical_feature' : categorical_features}, \n",
    "                                     scoring=scorer,\n",
    "                                     cv=3 # 3 fold cross validation(67% training, 33% evaluation)\n",
    "                                     ).mean()\n",
    "\n",
    "        return best_score\n",
    "    \n",
    "    print(\"Tuning parameter for Site\", i)\n",
    "    \n",
    "    best = fmin(fn=lgb_objective,\n",
    "                    space=lgb_space,\n",
    "                    max_evals=10,\n",
    "                    rstate=np.random.RandomState(42),\n",
    "                    algo=tpe.suggest)\n",
    "    \n",
    "    \n",
    "    print(\"Best Parameter for Site\", i, \": \", best)\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params[i] = best;\n",
    "    \n",
    "    # Write parameters to file\n",
    "    fname = \"best_params_\" + str(i) + \".txt\"\n",
    "    f = open(fname, \"w+\")\n",
    "    f.write(str(best))\n",
    "    f.close()\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_feather('test.feather')\n",
    "weather_test = pd.read_feather('weather_test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1272.51 MB\n",
      "Memory usage after optimization is: 358.65 MB\n",
      "Decreased by 71.8%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  building_id  meter            timestamp\n",
       "0       0            0      0  2017-01-01 00:00:00\n",
       "1       1            1      0  2017-01-01 00:00:00\n",
       "2       2            2      0  2017-01-01 00:00:00\n",
       "3       3            3      0  2017-01-01 00:00:00\n",
       "4       4            4      0  2017-01-01 00:00:00"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = reduce_mem_usage(test)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Merge Building Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.07 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 73.8%\n"
     ]
    }
   ],
   "source": [
    "building_meta = pd.read_feather('building_metadata.feather')\n",
    "building_meta = reduce_mem_usage(building_meta,use_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(building_meta, left_on='building_id', right_on='building_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del building_meta\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Fill Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 19.25 MB\n",
      "Memory usage after optimization is: 9.05 MB\n",
      "Decreased by 53.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>air_temperature</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>dew_temperature</th>\n",
       "      <th>precip_depth_1_hr</th>\n",
       "      <th>sea_level_pressure</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>1021.400024</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1022.000000</td>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1021.900024</td>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>17.200001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1022.200012</td>\n",
       "      <td>2017-01-01 03:00:00</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1022.299988</td>\n",
       "      <td>2017-01-01 04:00:00</td>\n",
       "      <td>130.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_id  air_temperature  cloud_coverage  dew_temperature  \\\n",
       "0        0        17.799999             4.0             11.7   \n",
       "1        0        17.799999             2.0             12.8   \n",
       "2        0        16.100000             0.0             12.8   \n",
       "3        0        17.200001             0.0             13.3   \n",
       "4        0        16.700001             2.0             13.3   \n",
       "\n",
       "   precip_depth_1_hr  sea_level_pressure            timestamp  wind_direction  \\\n",
       "0           0.282609         1021.400024  2017-01-01 00:00:00           100.0   \n",
       "1           0.000000         1022.000000  2017-01-01 01:00:00           130.0   \n",
       "2           0.000000         1021.900024  2017-01-01 02:00:00           140.0   \n",
       "3           0.000000         1022.200012  2017-01-01 03:00:00           140.0   \n",
       "4           0.000000         1022.299988  2017-01-01 04:00:00           130.0   \n",
       "\n",
       "   wind_speed  \n",
       "0         3.6  \n",
       "1         3.1  \n",
       "2         3.1  \n",
       "3         3.1  \n",
       "4         2.6  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_test = weather_pipeline(weather_test)\n",
    "weather_test = reduce_mem_usage(weather_test)\n",
    "weather_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Merge Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site_id</th>\n",
       "      <th>primary_use</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>year_built</th>\n",
       "      <th>floor_count</th>\n",
       "      <th>air_temperature</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>dew_temperature</th>\n",
       "      <th>precip_depth_1_hr</th>\n",
       "      <th>sea_level_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>7432</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>1021.400024</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>2720</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>1021.400024</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>5376</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>1021.400024</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>23685</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>1021.400024</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>116607</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>1021.400024</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  building_id  meter            timestamp  site_id primary_use  \\\n",
       "0       0            0      0  2017-01-01 00:00:00        0   Education   \n",
       "1       1            1      0  2017-01-01 00:00:00        0   Education   \n",
       "2       2            2      0  2017-01-01 00:00:00        0   Education   \n",
       "3       3            3      0  2017-01-01 00:00:00        0   Education   \n",
       "4       4            4      0  2017-01-01 00:00:00        0   Education   \n",
       "\n",
       "   square_feet  year_built  floor_count  air_temperature  cloud_coverage  \\\n",
       "0         7432      2008.0          NaN        17.799999             4.0   \n",
       "1         2720      2004.0          NaN        17.799999             4.0   \n",
       "2         5376      1991.0          NaN        17.799999             4.0   \n",
       "3        23685      2002.0          NaN        17.799999             4.0   \n",
       "4       116607      1975.0          NaN        17.799999             4.0   \n",
       "\n",
       "   dew_temperature  precip_depth_1_hr  sea_level_pressure  wind_direction  \\\n",
       "0             11.7           0.282609         1021.400024           100.0   \n",
       "1             11.7           0.282609         1021.400024           100.0   \n",
       "2             11.7           0.282609         1021.400024           100.0   \n",
       "3             11.7           0.282609         1021.400024           100.0   \n",
       "4             11.7           0.282609         1021.400024           100.0   \n",
       "\n",
       "   wind_speed  \n",
       "0         3.6  \n",
       "1         3.6  \n",
       "2         3.6  \n",
       "3         3.6  \n",
       "4         3.6  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.merge(weather_test, how='left',on=['timestamp','site_id'])\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del weather_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = FE_pipeline(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    best_params[i]['n_threads'] = 16\n",
    "    best_params[i]['metric'] = 'rmse'\n",
    "    best_params[i]['objective'] = 'regression'\n",
    "    best_params[i]['boosting'] = 'gbdt'\n",
    "    best_params[i]['max_depth'] = int(best_params[i]['max_depth'])\n",
    "    best_params[i]['num_leaves'] = int(best_params[i]['num_leaves'])\n",
    "    best_params[i]['min_data_in_leaf'] = int(best_params[i]['min_data_in_leaf'])\n",
    "    print(best_params[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the models\n",
    "models = {}\n",
    "for i in range(16):\n",
    "    models[i] = []\n",
    "\n",
    "# Dictionary to store evaluation results for later graphing\n",
    "eval_results = {}\n",
    "for i in range(16):\n",
    "    eval_results[i] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Fit Models by Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Make predictions based on sites\n",
    "for site_id in tqdm(range(16), desc='site_id'):\n",
    "    print(\"Training models for Site\", site_id)\n",
    "    \n",
    "    # Create a mask filtering site_id \n",
    "    site_mask = (X.site_id == site_id)\n",
    "    \n",
    "    # Partition training data into 3 parts with equal percentage for each meter type\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True) # 3-fold\n",
    "    fold = 0\n",
    "    for train_indices, test_indices in skf.split(X[site_mask], X[site_mask]['meter']):\n",
    "        print(\"Fold:\", fold)\n",
    "        fold += 1\n",
    "        \n",
    "        X_train = X[site_mask].iloc[train_indices]\n",
    "        y_train = y[site_mask].iloc[train_indices]\n",
    "\n",
    "        X_test = X[site_mask].iloc[test_indices]\n",
    "        y_test = y[site_mask].iloc[test_indices]\n",
    "\n",
    "        d_train = lgb.Dataset(X_train, \n",
    "                              label=y_train, \n",
    "                              categorical_feature=categorical_features, \n",
    "                              free_raw_data=False)\n",
    "        \n",
    "        d_test = lgb.Dataset(X_test, \n",
    "                             label=y_test, \n",
    "                             categorical_feature=categorical_features, \n",
    "                             free_raw_data=False)\n",
    "\n",
    "        \n",
    "        model = lgb.train(best_params[site_id], # The optimal parameters for each site \n",
    "                          train_set=d_train, \n",
    "                          num_boost_round=1000, \n",
    "                          valid_sets=[d_test, d_train],\n",
    "                          valid_names = ['validation', 'train'],\n",
    "                          verbose_eval=0, \n",
    "                          evals_result = eval_results[site_id][fold],\n",
    "                          early_stopping_rounds=50)\n",
    "        \n",
    "        models[site_id].append(model)\n",
    "        \n",
    "        del X_train, y_train, X_test, y_test, d_train, d_test\n",
    "        gc.collect()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for site_id in range(16):\n",
    "    print('Learning Curve for Site', site_id)\n",
    "    ax = lgb.plot_metric(eval_results[site_id], metric='rmse')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols.remove('row_id')\n",
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['meter'] = test['meter'].astype('category')\n",
    "test['meter'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sites = []\n",
    "for site_id in tqdm(range(16), desc=\"site_id\"):\n",
    "    test_site = test[test.site_id==site_id]\n",
    "    row_ids_site = test_site.row_id\n",
    "\n",
    "    test_site = test_site[input_cols]\n",
    "    y_pred = np.zeros(test_site.shape[0])\n",
    "\n",
    "    print(\"Predicting meter reading for Site\", site_id)    \n",
    "    for fold in range(3):\n",
    "        model = models[site_id][fold]\n",
    "        # We take the average of the prediction result of the 3 models\n",
    "        y_pred += model.predict(test_site, \n",
    "                                          num_iteration=model.best_iteration) / 3\n",
    "        gc.collect()\n",
    "        \n",
    "    df_test_site = pd.DataFrame({\"row_id\": row_ids_site, \n",
    "                                 \"meter_reading\": y_pred})\n",
    "    df_test_sites.append(df_test_site)\n",
    "    \n",
    "    print(\"Prediction for site_id\", site_id, \"completed\\n\")\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "submit = pd.concat(df_test_sites)\n",
    "submit.meter_reading = np.clip(np.expm1(submit.meter_reading), 0, a_max=None)\n",
    "submit.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
